# whispr

**Transcribe â†’ Summarize â†’ Understand**  
Whispr ist eine Dockerized Pipeline, die Videos automatisch in Text und Zusammenfassungen umwandelt.  
Sie nutzt [OpenAI Whisper](https://github.com/openai/whisper) fÃ¼r die Transkription und LLMs (OpenAI / OpenRouter) fÃ¼r die Zusammenfassung.

---

## âœ¨ Features

- ğŸ¬ **Video â†’ Audio**: `.mp4` â†’ `.mp3` mit `ffmpeg`
- ğŸ§ **Audio â†’ Transcript**: Transkription mit Whisper (GPU-beschleunigt, falls verfÃ¼gbar)
- ğŸ“ **Transcript â†’ Summary**: Automatische Zusammenfassungen mit OpenAI, OpenRouter oder LM Studio
- ğŸ³ **Dockerized**: Einfache Nutzung mit GPU-Support (zurzeit nur von OP mit NVIDIA GPU's getestet)
- âš¡ **Flags & Workflows**: Flexible AusfÃ¼hrung mit `--only-transcript` oder Standard-Workflow

---

## ğŸ“‚ Projektstruktur

```
.
â”œâ”€â”€ transcribe.py         # Hauptpipeline
â”œâ”€â”€ transcribe_api.py     # Flask API fÃ¼r Uploads
â”œâ”€â”€ run_whispr.sh         # Script fÃ¼r Docker build & run
â”œâ”€â”€ Dockerfile            # Containerdefinition
â”œâ”€â”€ video/                # Inputvideos (.mp4)
â”œâ”€â”€ audio/                # Extrahiertes Audio (.mp3)
â”œâ”€â”€ transcripts/          # Transkripte (.txt)
â”œâ”€â”€ summaries/            # Zusammenfassungen (.txt)
â”œâ”€â”€ prompt.txt            # Optionaler Custom-Prompt
â”œâ”€â”€ .env                  # API Keys
```

---

## âš¡ Quickstart

### 1. Clone & Build
```bash
git clone https://github.com/yourusername/whispr.git
cd whispr
./setup.sh # erstellt benÃ¶tigte ordner video/, audio/, transcripts/, summaries/
```

ğŸ‘‰ Baut das Docker-Image `whispr` und startet den Container mit allen notwendigen Mounts.

---

### 2. Videos hinzufÃ¼gen
Lege deine `.mp4` Dateien im Ordner `video/` ab.

---

### 3. Environment konfigurieren
Erstelle eine `.env` Datei mit deinen Keys:

```ini
OPENAI_API_KEY=sk-xxxx
OPENROUTER_API_KEY=or-xxxx
```

WÃ¤hle den Service in `transcribe.py`:
```python
SERVICE = "openai"       # "openai", "open-router", or "lm-studio"
```

WÃ¤hle das gewollte AI Modell in `transcribe.py`:
```python
MODEL_NAME = "gpt-5"       # (By Default its set to an openai API Model)
```

---

### 4. Prompt anpassen
Gegebenenfals prompt.txt nach vorlieben anpassen

---

### 5. Pipeline ausfÃ¼hren
Standard (Video â†’ Audio â†’ Transcript â†’ Summary):
```bash
./run_whispr.sh
```
Nur bis Transkript (keine API-Zusammenfassung):
```bash
./run_whispr.sh --only-transcript
```
Hilfe anzeigen:
```bash
./run_whispr.sh -h
```

Die Pipeline konvertiert automatisch:

1. `.mp4` â†’ `.mp3`
2. `.mp3` â†’ Transkript (`transcripts/`)
3. Transkript â†’ Zusammenfassung (`summaries/`)

---

## ğŸ›  Dependencies

- Python 3.10
- [PyTorch](https://pytorch.org/) (CUDA falls verfÃ¼gbar)
- [Whisper](https://github.com/openai/whisper)
- ffmpeg
- requests
- python-dotenv

ğŸ‘‰ Alles wird automatisch im Docker-Container installiert.

---

## ğŸ“Œ Roadmap

- [ ] Mehr Whisper-Modelle (`tiny`, `small`, `medium`, `large`)
- [ ] Leichtere Konfiguration von genutzten AI-Modellen
- [ ] Verbessertes docker-handling
- [ ] Gegebenenfalls automatische AusfÃ¼hrung bei Ablage in Ordner

---

## ğŸ“„ License

MIT License â€” frei nutzbar, verÃ¤nderbar und teilbar.
